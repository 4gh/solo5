            |            
  __|  _ \  |  _ \   __| 
\__ \ (   | | (   |\__ \ 
____/\___/ _|\___/ ____/ 

How to debug with QEMU + GDB
- this is tricky because gdb doesn't like the mode switching between
  the 32-bit loader and the 64-bit kernel.  So we basically need to
  get the kernel to a point where we can attach where the kernel is
  already in 64-bit mode.  My trick is to have an infinite loop that
  we break by setting a volatile int variable.
- start qemu with -s
    qemu-system-x86_64 -s -nographic -cdrom kernel.iso
    -or-
    make qemu
- in another terminal, start gdb
    gdb kernel/kernel
    target remote localhost:1234
    set gdb=1
    c
- or I have that in a command file
    gdb kernel/kernel --command gdb.txt


How to find where an interrupt came from in gdb:
x/1x interrupt_stack_page+4040


Assuming KVM will create an interface veth0 and add it to an existing
virbr0, here is how to set up the network for pinging on a private
bridge:
    sudo ip link add veth1 type veth peer name veth2
    sudo ifconfig veth1 10.0.0.1/24 up
    sudo ifconfig veth2 up
    sudo brctl addif virbr0 veth2

And here is how to forward webserver traffic for the mirage webserver:
    iptables -t nat -A POSTROUTING ! -d 10.0.0.0/24 -o eth0 -j SNAT --to-source 9.2.252.208
    iptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j DNAT --to 10.0.0.2:80
~.
    iptables -A FORWARD -p tcp -d 10.0.0.2 --dport 80 -j ACCEPT


Note: you need to add the arp entry to the host because the kernel
doesn't respond to arps
    sudo arp -s 10.0.4.10 52:54:00:12:34:56

Getting a series of patchs from the master branch:
Assume that tag A is the commit on master before the one we want.
Assume that tag B is the commit on master of the final patch in the series. 
  git checkout mirage
  git checkout -b tmp
  git branch -f mirage B
  git rebase --onto tmp A mirage
  git branch -d tmp


Todos:
- cleanup
  - remove old printk
  - get rid of memcheck
  - sort out loader mess
    - clear bss in loader
    - merge common mm routines in kernel and loader
    - merge common lib in kernel and loader
    - look into lkvm
- consolidate virtq stuff
- protect bottom of stack
- use larger pages



The Solo5 Boot Process

Solo5 is packaged as an iso (kernel.iso).  Inside this iso is a
version of the GRUB bootloader (stage2_eltorito), a GRUB configuration
file (menu.lst), a 32-bit elf file (loader) and a 64-bit elf file
(kernel).  The reason for the 32-bit loader is that when Solo5 was
developed, GRUB could not directly boot into the 64-bit kernel.  The
GRUB configuration file specifies loader as the "kernel" and kernel as
the first "module".

We don't really care about the memory map too much at this point,
except for the fact that GRUB loads the loader at 1M, as specified in
the loader.ld file.  GRUB puts the first module (the kernel) after the
loader on a 4K page boundary with an extra two full 4K pages of
padding.  The kernel's linker file is dynamically generated by
gen_ld.bash so that the kernel binary is consistent with the address
that GRUB loads it to even if the size of the loader changes.  So,
after GRUB, the part of the memory map we care about looks like this,
with just a bit of space between the loader and kernel:

|----|-- loader ---|-- kernel --|----------------------------------|
   0x100000     0x108000

GRUB will jump to execute the loader first, at _start in boot.s.  In a
bit more detail, the loader layout looks like this (specified in
loader.ld)

|--multiboot--text--data--bootstrap_stack--|

The first file to execute is boot.s in the loader.  Here you see the
multiboot section, which contains the multiboot header (magic number,
etc).  The bootstrap stack is specified as 16K big.  When _start is
called, it simply sets the stack pointer to the space allocated for
the bootstrap stack (for a known good stack), and calls loader_main
(in loader.c) with a pointer to the multiboot structure (from GRUB) as
an argument.

loader_main gets a couple of things from the multiboot structure: the
maximum available memory, the memory address marking the end of the
kernel in memory, and the address of the kernel entry point.  The
first two are used to set up the page tables (which must be loaded
before the jump to 64-bit mode).  The pagetable_init function in
page_tables.c directly maps all pages and puts the page table
structures just after the kernel in memory.  These mappings are not
bootstrap mappings; they will be used by the 64-bit kernel.  The
memory map is now:

|----|-loader/bootstrap_stack-|-kernel-|-pagetables-|---------------|

loader_main next initializes a bootstrap GDT, which is necessary for
the jump to 64-bit mode.  The bootstrap GDT only contains a code and
data segment.  The 64-bit kernel adds the TSS entries later, when it
re-initializes the GDT.

|----|-loader/bootstrap_GDT/bootstrap_stack-|-kernel-|-pagetables-|--|

Next, the processor is configured for long mode.  to64_prep in to64.s
sets the pml4 as the base page table pointer, enables PAE and long
compat mode, enables paging, and finally loads the bootstrap GDT.
to64_jump switches to long mode (64-bit), sets the stack to the top of
memory (we are no longer using the loader's bootstrap stack) and jumps
to the kernel entry point that was previously read from the multiboot
structure.  Memory now looks like this:

|----|-loader/bootstrap_GDT-|-kernel-|-pagetables-|------------stack|

The kernel entry point is kernel_main in kernel.c, as specified in
kernel.ld.  This is now in the kernel directory, where everything is
64-bit.  We initialize the serial device immediately, so that we can
use printf (which uses serial output) for debugging.  We also have a
loop very early on a volatile integer named "gdb".  This is a
convenient way to allow us to connect gdb (through qemu) in pure 64
bit mode, rather than having gdb become confused by the jump from 32
to 64.

kernel_main is passed a pointer to the multiboot structure, which we
use for the remaining memory initialization in mem_init (mem.c).
mem_init must be called early because it clears the bss (the location
of which is specified in the linker file).  After that, the main task
of mem_init is to set up a bitmap for page-level memory allocation.
It traverses the multiboot structure (again!) to get the kernel start
and end, and calculates where the loader should have put the page
tables.  (This is not ideal - the kernel shouldn't be guessing about
what the loader did).  The kernel then creates a bitmap to keep track
of what pages are available.  Starting with all pages unavailable, the
kernel traverses the multiboot memory map and marks available pages as
available unless they are used by the kernel, the page table area, or
the bitmap area.  It then also marks the stack pages as unavailable.
The memory after the bitmap area is recorded of the start of the heap.

|-|-loader/boot_GDT-|-kernel-|-pagetables-|-bitmap-|-heap->---<-stack|

There is a bug here!!!  The bootstrap GDT is in an "available" area.
In practice this isn't a problem, as the heap doesn't know about that
memory anyway, and all memory allocation (malloc/sbrk) will be through
the heap.  In any case, this is not ideal, because all memory under
the kernel is wasted.

Next, kernel_main calls interrupts_init in interrupts.c.  This code
initializes the TSS, re-initializes the GDT, and initializes the IDT.
The main part of the TSS is that it sets up a "known-good" interrupt
stack to avoid double faults on stack issues.  At some point I want to
page-protect the bottom of the stack and demand-page it.  The GDT
includes an entry to the TSS; this is really why we need to
reinitialize it.  It may be cleaner to set up the GDT once, in the
loader, complete with the interrupt stack for the TSS entry, as we
don't expect to touch the GDT or TSS ever again.  At this point, we
really don't care about the loader in the memory map.

|-----|-kernel/GDT/TSS/IDT/-|-pagetables-|-bitmap-|-heap->---<-stack|

interrupts_init also sets up the IDT, which contains the interrupt
vectors for processor exceptions and (remapped) irq lines.  The entry
points for the interrupt service routines are in interrupts.s.  To
maintain a single set of constants for both the .c and .s interrupt
code, the header file interrupts.h is used.  It can be included
directly in the .c file, but is processed into interrupt_vectors.h for
the .s file to include via gen_interrupts.bash.

The interrupt code includes a nasty hack called the SS hack.  For
64-bit execution, we want to have a NULL SS (segments aren't used
anyway and seem to confuse the CPU on returning from interrupts).  To
force the SS to be NULL, we change the SS value on the stack before
returning from the first interrupt at setup time (INTR_SS_HACK)

Floating point is enabled via sse_enable, which is found in mem.s (but
doesn't seem to belong there).

The time keeping system is initialized using time_init in time.c.
Time is maintained via rdtsc.  We assume one CPU and a reliable tsc.
The speed of the CPU (GHZ) is configured into a macro in time.c.  We
may need to use a clock source (e.g., the PIT) to calibrate the TSC.






How to gauge the effort to port an application (in c)

1. Makefile needs to build an app_solo5.o object, e.g.,

mujs_solo5.o: $(MUJS_FILES)
       $(GCC64) $(MUJS_CFLAGS64) -c $(MUJS_DIR)/one.c -o mujs_solo5.o

2. Get rid of all standard headers and replace with kernel.h

3. Try to build and grab all implicit declarations and undeclared
   variables (look at gen_js_undefined.bash)


